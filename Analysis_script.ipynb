{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Authors: LÃ©andre Dubey\n",
    ": creation: 07.04.2024\n",
    "and Olivia Lecomte\n",
    ": modified on: 08.05.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:09:01.040197Z",
     "start_time": "2024-05-08T09:09:01.021079Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install statsmodels\n",
    "# !{sys.executable} -m pip install pyedflib\n",
    "# !{sys.executable} -m pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:09:03.461693Z",
     "start_time": "2024-05-08T09:09:01.042248Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# todo: update root folder\n",
    "root_folder = r'.\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EMFIT19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:09:03.477698Z",
     "start_time": "2024-05-08T09:09:03.463756Z"
    }
   },
   "outputs": [],
   "source": [
    "#Just for emfit19\n",
    "def process_file_emfit19(file_path, participant_id, plot=False):\n",
    "    participant_id = 'SMS_020'\n",
    "    file_path = os.path.join(root_folder, participant_id, \"EMFIT_001519\", \"SMS_020-psg-EMFIT_001519-processed_vitals.csv\")\n",
    "    df_emfit19 = pd.read_csv(file_path)\n",
    "    df_emfit19['timestamp'] = df_emfit19['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "    df_emfit19_filtered = df_emfit19[(df_emfit19['timestamp'].dt.hour >= 22) | (df_emfit19['timestamp'].dt.hour < 7)]\n",
    "    df_emfit19_filtered.set_index('timestamp', inplace=True)\n",
    "\n",
    "    normalized_activity = (df_emfit19_filtered['act'] - df_emfit19_filtered['act'].min()) / (df_emfit19_filtered['act'].max() - df_emfit19_filtered['act'].min())\n",
    "\n",
    "    if plot:\n",
    "        # Plot 'act' against 'timestamp'\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df_emfit19_filtered.index, normalized_activity)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Normalized Activity Level')\n",
    "        ax.set_title(f'Normalized Activity Level Between 22:00 and 07:00 for emfit19 Participant {participant_id}')\n",
    "\n",
    "        # Set x-axis ticks as hours\n",
    "        hours = HourLocator(interval=1)\n",
    "        hour_format = DateFormatter(\"%H:%M\")\n",
    "        ax.xaxis.set_major_locator(hours)\n",
    "        ax.xaxis.set_major_formatter(hour_format)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return df_emfit19_filtered.index, normalized_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Polysomnography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:09:03.492902Z",
     "start_time": "2024-05-08T09:09:03.478801Z"
    }
   },
   "outputs": [],
   "source": [
    "#Just for psg\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg(file_path, participant_id, plot=False):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip the first line\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    data = {'time': [], 'activity': []}\n",
    "    for data_line in data_lines:\n",
    "        data['time'].append(data_line[0])\n",
    "        # Remove any non-numeric characters from the activity data\n",
    "        activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "        data['activity'].append(activity)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    try:\n",
    "        datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    except ValueError:\n",
    "        datetime_format = '%H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Filter rows where the time is between 22:00 and 07:00\n",
    "    filtered_df = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # Convert 'activity' column to numeric type\n",
    "    filtered_df.loc[:,'activity'] = pd.to_numeric(filtered_df.loc[:,'activity'])\n",
    "\n",
    "    # Normalize the 'activity' data\n",
    "    normalized_activity = (filtered_df['activity'] - filtered_df['activity'].min()) / (\n",
    "                filtered_df['activity'].max() - filtered_df['activity'].min())\n",
    "\n",
    "    if plot:\n",
    "        # Plotting\n",
    "        # todo: add subplot axes to plot side by side or plot separately\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(filtered_df.index, normalized_activity)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Normalized Activity Level')\n",
    "        ax.set_title(f'Normalized Activity Level Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "\n",
    "        # Set x-axis ticks as hours\n",
    "        hours = HourLocator(interval=1)\n",
    "        hour_format = DateFormatter(\"%H:%M\")\n",
    "        ax.xaxis.set_major_locator(hours)\n",
    "        ax.xaxis.set_major_formatter(hour_format)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return filtered_df.index, normalized_activity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Polysomnography and EMFIT19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:09:33.660122Z",
     "start_time": "2024-05-08T09:09:03.495298Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main loop to process files\n",
    "for folder in os.listdir(root_folder):\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    if os.path.exists(os.path.join(root_folder, participant_id, \"Somnomedics\")):\n",
    "        file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "        x_psg, y_psg = process_file_psg(file_path_psg, participant_id)\n",
    "        fig, ax = plt.subplots(1, 2, figsize= (18,6))\n",
    "        ax[0].plot(x_psg, y_psg)\n",
    "        ax[0].set_xlabel('Time')\n",
    "        ax[0].set_ylabel('Normalized Activity Level')\n",
    "        ax[0].set_title(f'Normalized Activity Level (PSG) Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "\n",
    "        # Set x-axis ticks as hours\n",
    "        hours = HourLocator(interval=1)\n",
    "        hour_format = DateFormatter(\"%H:%M\")\n",
    "        ax[0].xaxis.set_major_locator(hours)\n",
    "        ax[0].xaxis.set_major_formatter(hour_format)\n",
    "        ax[0].grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    if os.path.exists(os.path.join(root_folder, participant_id, \"EMFIT_001519\")):\n",
    "        file_path_emfit19 = os.path.join(root_folder, participant_id, \"EMFIT_001519\", \"SMS_020-psg-EMFIT_001519-processed_vitals.csv\")\n",
    "        x_emfit19, y_emfit_19 = process_file_emfit19(file_path_psg, participant_id)\n",
    "        ax[1].plot(x_emfit19, y_emfit_19)\n",
    "        ax[1].set_xlabel('Time')\n",
    "        ax[1].set_ylabel('Normalized Activity Level')\n",
    "        ax[1].set_title(f'Normalized Activity Level (EMFIT19) Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "\n",
    "        # Set x-axis ticks as hours\n",
    "        hours = HourLocator(interval=1)\n",
    "        hour_format = DateFormatter(\"%H:%M\")\n",
    "        ax[1].xaxis.set_major_locator(hours)\n",
    "        ax[1].xaxis.set_major_formatter(hour_format)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Polysomnography over EMFIT 19 and cross-correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot emfit19 on top of psg\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "from scipy import signal\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg_emfit19(file_path_emfit19,file_path_psg, participant_id):\n",
    "    try:\n",
    "        df_emfit19 = pd.read_csv(file_path_emfit19)\n",
    "        df_emfit19['timestamp'] = df_emfit19['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "        df_emfit19_filtered = df_emfit19[(df_emfit19['timestamp'].dt.hour >= 22) | (df_emfit19['timestamp'].dt.hour < 7)]\n",
    "        df_emfit19_filtered.set_index('timestamp', inplace=True)\n",
    "\n",
    "        normalized_activity_emfit19 = (df_emfit19_filtered['act'] - df_emfit19_filtered['act'].min()) / (df_emfit19_filtered['act'].max() - df_emfit19_filtered['act'].min())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for emfit 19, participant {participant_id}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path_psg, 'r') as file:\n",
    "        # Skip the first line\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    data = {'time': [], 'activity': []}\n",
    "    for data_line in data_lines:\n",
    "        data['time'].append(data_line[0])\n",
    "        # Remove any non-numeric characters from the activity data\n",
    "        activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "        data['activity'].append(activity)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    try:\n",
    "        datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    except ValueError:\n",
    "        datetime_format = '%H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Filter rows where the time is between 22:00 and 07:00\n",
    "    filtered_df_psg = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # Convert 'activity' column to numeric type\n",
    "    filtered_df_psg.loc[:,'activity'] = pd.to_numeric(filtered_df_psg.loc[:,'activity'])\n",
    "\n",
    "    # Normalize the 'activity' data\n",
    "    normalized_activity_psg = (filtered_df_psg['activity'] - filtered_df_psg['activity'].min()) / (\n",
    "                filtered_df_psg['activity'].max() - filtered_df_psg['activity'].min())\n",
    "\n",
    "\n",
    "    normalized_activity_psg = pd.to_numeric(normalized_activity_psg)\n",
    "    normalized_activity_emfit19 = pd.to_numeric(normalized_activity_emfit19)\n",
    "\n",
    "    norm_psg_values = normalized_activity_psg.dropna().values\n",
    "    norm_emfit19_values = normalized_activity_emfit19.values\n",
    "\n",
    "\n",
    "    # todo: add cross correlations using scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html)\n",
    "    # todo: realign signals\n",
    "    # Determine the length of the aligned signals\n",
    "    max_len = max(len(norm_psg_values), len(norm_emfit19_values))\n",
    "    \n",
    "    # # Pad signals to the same length\n",
    "    # psg_padded = np.pad(norm_psg_values, (0, max_len - len(norm_psg_values)), mode='constant')\n",
    "    # emfit19_padded = np.pad(norm_emfit19_values, (0, max_len - len(norm_emfit19_values)), mode='constant')\n",
    "    \n",
    "    # Cross-correlation\n",
    "    corr = signal.correlate(norm_psg_values, norm_emfit19_values, mode='same')\n",
    "    lags = signal.correlation_lags(len(norm_psg_values), len(norm_emfit19_values), mode='same')\n",
    "    corr /= np.max(corr)\n",
    "    \n",
    "    # max_corr = np.max(corr)\n",
    "    \n",
    "    # Find the index corresponding to the maximum correlation coefficient\n",
    "    max_corr_index = np.argmax(corr)\n",
    "    lag_for_alignment = lags[max_corr_index]\n",
    "\n",
    "    # Alignment\n",
    "    if lag_for_alignment > 0:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_emfit19_values = np.pad(norm_emfit19_values, (lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "    elif lag_for_alignment < 0:\n",
    "        aligned_psg_values = np.pad(norm_psg_values, (-lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "        aligned_emfit19_values = norm_emfit19_values[:max_len]\n",
    "    else:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_emfit19_values = norm_emfit19_values[:max_len]\n",
    "        \n",
    "        \n",
    "    # cross-correlation\n",
    "    corr_a = signal.correlate(aligned_psg_values, aligned_emfit19_values, mode='same')\n",
    "    lags_a = signal.correlation_lags(len(aligned_psg_values), len(aligned_emfit19_values), mode='same')\n",
    "    corr_a /= np.max(corr_a)\n",
    "\n",
    "    print(f'Participant {participant_id}\\nCorrelation without re-alignment (correlation at lags = 0) {corr[lags==0]}\\nCorrelation after re-alignment (correlation at lags = 0) {corr_a[lags_a==0]}')  # Print mean correlation\n",
    "\n",
    "    all_corr_wo_alignment.append(corr[lags==0])\n",
    "    all_corr_after_alignment.append(corr_a[lags_a==0])\n",
    "\n",
    "\n",
    "    # Calculate mean correlation\n",
    "    # mean_corr = np.mean(corr_a) # doesn't make sense to do this\n",
    "    # print(f'Participant {participant_id}\\nMaximum Correlation {max_corr}\\nMean Correlation {mean_corr} for emfit19')  # Print mean correlation\n",
    "\n",
    "    # emfit19_corr.append(mean_corr)\n",
    "\n",
    "    # todo: plots\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(1,3, figsize = (18, 6))\n",
    "    ax[0].plot(df_emfit19_filtered.index, normalized_activity_emfit19, color='orange', label='emfit19')\n",
    "    ax[0].plot(filtered_df_psg.index, normalized_activity_psg, color='blue', label='PSG')\n",
    "    \n",
    "    ax[1].plot(lags, corr)\n",
    "\n",
    "    ax[2].plot(lags_a, corr_a)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Normalized Activity Level')\n",
    "    ax[0].set_title(f'Normalized Activity Level Between 22:00 and 07:00 \\nfor Participant {participant_id}')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Set x-axis ticks to every hour\n",
    "    hours = HourLocator(interval=1)\n",
    "    hour_format = DateFormatter(\"%H:%M\")\n",
    "    ax[0].xaxis.set_major_locator(hours)\n",
    "    ax[0].xaxis.set_major_formatter(hour_format)\n",
    "    # plt.xticks(rotation=45)\n",
    "\n",
    "    # set title and labels\n",
    "    ax[1].set_title(f'Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[1].set_xlabel('Lag')\n",
    "    ax[1].set_ylabel('r')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax[2].set_title(f'Aligned Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[2].set_xlabel('Lag')\n",
    "    ax[2].set_ylabel('r')\n",
    "\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Main loop to process files\n",
    "emfit19_corr = []\n",
    "total_participants = 0\n",
    "has_psg_and_emfit19_folder = 0\n",
    "all_corr_wo_alignment = []\n",
    "all_corr_after_alignment = []\n",
    "\n",
    "for folder in os.listdir(root_folder):\n",
    "    total_participants += 1\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "    file_path_emfit19 = os.path.join(root_folder, participant_id, \"EMFIT_001519\", f\"{participant_id}-psg-EMFIT_001519-processed_vitals.csv\")\n",
    "    problematic_patients = ['SMS_114','SMS_113','SMS_111','SMS_098','SMS_092','SMS_043']\n",
    "    if os.path.exists(file_path_emfit19) and os.path.exists(file_path_psg) and participant_id not in problematic_patients:\n",
    "        process_file_psg_emfit19(file_path_emfit19, file_path_psg, participant_id)\n",
    "        has_psg_and_emfit19_folder +=1\n",
    "    elif participant_id in problematic_patients: print(f'Problematic patient encountered: {participant_id}')\n",
    "    else:\n",
    "        print(f\"Files missing for participant {participant_id}\")\n",
    "\n",
    "# print(f'Average correlation for EMFIT19 is {np.mean(emfit19_corr)}')\n",
    "print(f'Total number of participants:{total_participants}\\nTotal number of participants compared for PSG and emfit19: {has_psg_and_emfit19_folder}')\n",
    "print(f'Number of omitted participants: {len(problematic_patients)}')\n",
    "print(f'Average correlation before alignment: {np.mean(all_corr_wo_alignment)}, std: {np.std(all_corr_wo_alignment)}')\n",
    "print(f'Average correlation after alignment: {np.mean(all_corr_after_alignment)}, std: {np.std(all_corr_after_alignment)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Polysomnography over EMFIT 05 and cross-correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:10:16.454851Z",
     "start_time": "2024-05-08T09:09:56.053176Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot emfit05 on top of psg\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg_emfit05(file_path_emfit05, file_path_psg, participant_id):\n",
    "    try:\n",
    "        df_emfit05 = pd.read_csv(file_path_emfit05)\n",
    "        df_emfit05['timestamp'] = df_emfit05['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "        df_emfit05_filtered = df_emfit05[\n",
    "            (df_emfit05['timestamp'].dt.hour >= 22) | (df_emfit05['timestamp'].dt.hour < 7)]\n",
    "        df_emfit05_filtered.set_index('timestamp', inplace=True)\n",
    "\n",
    "        normalized_activity_emfit05 = (df_emfit05_filtered['act'] - df_emfit05_filtered['act'].min()) / (\n",
    "                    df_emfit05_filtered['act'].max() - df_emfit05_filtered['act'].min())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for emfit 05, participant {participant_id}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path_psg, 'r') as file:\n",
    "        # Skip the first line\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    data = {'time': [], 'activity': []}\n",
    "    for data_line in data_lines:\n",
    "        data['time'].append(data_line[0])\n",
    "        # Remove any non-numeric characters from the activity data\n",
    "        activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "        data['activity'].append(activity)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    try:\n",
    "        datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    except ValueError:\n",
    "        datetime_format = '%H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Filter rows where the time is between 22:00 and 07:00\n",
    "    filtered_df_psg = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # Convert 'activity' column to numeric type\n",
    "    filtered_df_psg.loc[:, 'activity'] = pd.to_numeric(filtered_df_psg.loc[:, 'activity'])\n",
    "\n",
    "    # Normalize the 'activity' data\n",
    "    normalized_activity_psg = (filtered_df_psg['activity'] - filtered_df_psg['activity'].min()) / (\n",
    "            filtered_df_psg['activity'].max() - filtered_df_psg['activity'].min())\n",
    "\n",
    "\n",
    "    normalized_activity_psg = pd.to_numeric(normalized_activity_psg)\n",
    "    normalized_activity_emfit05 = pd.to_numeric(normalized_activity_emfit05)\n",
    "\n",
    "    norm_psg_values = normalized_activity_psg.dropna().values\n",
    "    norm_emfit05_values = normalized_activity_emfit05.values\n",
    "\n",
    "    # todo: add cross correlations using scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html)\n",
    "    # todo: realign signals\n",
    "    # Determine the length of the aligned signals\n",
    "    max_len = max(len(norm_psg_values), len(norm_emfit05_values))\n",
    "    \n",
    "    # # Pad signals to the same length\n",
    "    # psg_padded = np.pad(norm_psg_values, (0, max_len - len(norm_psg_values)), mode='constant')\n",
    "    # emfit05_padded = np.pad(norm_emfit05_values, (0, max_len - len(norm_emfit05_values)), mode='constant')\n",
    "    \n",
    "    # Cross-correlation\n",
    "    corr = signal.correlate(norm_psg_values, norm_emfit05_values, mode='same')\n",
    "    lags = signal.correlation_lags(len(norm_psg_values), len(norm_emfit05_values), mode='same')\n",
    "    corr /= np.max(corr)\n",
    "    \n",
    "    # max_corr = np.max(corr)\n",
    "    \n",
    "    # Find the index corresponding to the maximum correlation coefficient\n",
    "    max_corr_index = np.argmax(corr)\n",
    "    lag_for_alignment = lags[max_corr_index]\n",
    "\n",
    "    # Alignment\n",
    "    if lag_for_alignment > 0:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_emfit05_values = np.pad(norm_emfit05_values, (lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "    elif lag_for_alignment < 0:\n",
    "        aligned_psg_values = np.pad(norm_psg_values, (-lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "        aligned_emfit05_values = norm_emfit05_values[:max_len]\n",
    "    else:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_emfit05_values = norm_emfit05_values[:max_len]\n",
    "        \n",
    "        \n",
    "    # cross-correlation\n",
    "    corr_a = signal.correlate(aligned_psg_values, aligned_emfit05_values, mode='same')\n",
    "    lags_a = signal.correlation_lags(len(aligned_psg_values), len(aligned_emfit05_values), mode='same')\n",
    "    corr_a /= np.max(corr_a)\n",
    "\n",
    "    print(f'Participant {participant_id}\\nCorrelation without re-alignment (correlation at lags = 0) {corr[lags==0]}\\nCorrelation after re-alignment (correlation at lags = 0) {corr_a[lags_a==0]}')  # Print mean correlation\n",
    "\n",
    "    all_corr_wo_alignment_emfit05.append(corr[lags==0])\n",
    "    all_corr_after_alignment_emfit05.append(corr_a[lags_a==0])\n",
    "\n",
    "\n",
    "    # todo: plots\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(1,3, figsize = (18, 6))\n",
    "    ax[0].plot(df_emfit05_filtered.index, normalized_activity_emfit05, color='orange', label='emfit05')\n",
    "    ax[0].plot(filtered_df_psg.index, normalized_activity_psg, color='blue', label='PSG')\n",
    "    \n",
    "    ax[1].plot(lags, corr)\n",
    "\n",
    "    ax[2].plot(lags_a, corr_a)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Normalized Activity Level')\n",
    "    ax[0].set_title(f'Normalized Activity Level Between 22:00 and 07:00 \\nfor Participant {participant_id}')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Set x-axis ticks to every hour\n",
    "    hours = HourLocator(interval=1)\n",
    "    hour_format = DateFormatter(\"%H:%M\")\n",
    "    ax[0].xaxis.set_major_locator(hours)\n",
    "    ax[0].xaxis.set_major_formatter(hour_format)\n",
    "    # plt.xticks(rotation=45)\n",
    "\n",
    "    # set title and labels\n",
    "    ax[1].set_title(f'Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[1].set_xlabel('Lag')\n",
    "    ax[1].set_ylabel('r')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax[2].set_title(f'Aligned Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[2].set_xlabel('Lag')\n",
    "    ax[2].set_ylabel('r')\n",
    "\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main loop to process files\n",
    "emfit05_corr = []\n",
    "total_participants = 0\n",
    "has_psg_and_emfit05_folder = 0\n",
    "all_corr_wo_alignment_emfit05 = []\n",
    "all_corr_after_alignment_emfit05 = []\n",
    "\n",
    "for folder in os.listdir(root_folder):\n",
    "    total_participants += 1\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "    file_path_emfit05 = os.path.join(root_folder, participant_id, \"EMFIT_001505\",\n",
    "                                     f\"{participant_id}-psg-EMFIT_001505-processed_vitals.csv\")\n",
    "    problematic_patients = ['SMS_175','SMS_173','SMS_170','SMS_161', 'SMS_114', 'SMS_113', 'SMS_111', 'SMS_098', 'SMS_092', 'SMS_043']\n",
    "    if os.path.exists(file_path_emfit05) and os.path.exists(\n",
    "            file_path_psg) and participant_id not in problematic_patients:\n",
    "        process_file_psg_emfit05(file_path_emfit05, file_path_psg, participant_id)\n",
    "        has_psg_and_emfit05_folder +=1\n",
    "    elif participant_id in problematic_patients:\n",
    "        print(f'Problematic patient encountered: {participant_id}')\n",
    "    else:\n",
    "        print(f\"Files missing for participant {participant_id}\")\n",
    "\n",
    "# print(f'Average correlation for EMFIT05 is {np.mean(emfit05_corr)}')\n",
    "print(f'Total number of participants:{total_participants}\\nTotal number of participants compared for PSG and emfit05: {has_psg_and_emfit19_folder}')\n",
    "print(f'Number of omitted participants: {len(problematic_patients)}')\n",
    "print(f'Average correlation before alignment: {np.mean(all_corr_wo_alignment_emfit05)}, std: {np.std(all_corr_wo_alignment_emfit05)}')\n",
    "print(f'Average correlation after alignment: {np.mean(all_corr_after_alignment_emfit05)}, std: {np.std(all_corr_after_alignment_emfit05)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Polysomnograph over EMFIT 05 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:10:30.100050Z",
     "start_time": "2024-05-08T09:10:16.455957Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot emfit05 on top of psg\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg_emfit05(file_path_emfit05,file_path_psg, participant_id):\n",
    "    try:\n",
    "        df_emfit05 = pd.read_csv(file_path_emfit05)\n",
    "        df_emfit05['timestamp'] = df_emfit05['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "        df_emfit05_filtered = df_emfit05[(df_emfit05['timestamp'].dt.hour >= 22) | (df_emfit05['timestamp'].dt.hour < 7)]\n",
    "        df_emfit05_filtered.set_index('timestamp', inplace=True)\n",
    "\n",
    "        normalized_activity_emfit05 = (df_emfit05_filtered['act'] - df_emfit05_filtered['act'].min()) / (df_emfit05_filtered['act'].max() - df_emfit05_filtered['act'].min())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for emfit 05, participant {participant_id}\")\n",
    "        return\n",
    "\n",
    "    # with open(file_path_psg, 'r') as file:\n",
    "    #     # Skip the first line\n",
    "    #     next(file)\n",
    "    #     lines = file.readlines()\n",
    "\n",
    "    # data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    # data = {'time': [], 'activity': []}\n",
    "    # for data_line in data_lines:\n",
    "    #     data['time'].append(data_line[0])\n",
    "    #     # Remove any non-numeric characters from the activity data\n",
    "    #     activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "    #     data['activity'].append(activity)\n",
    "\n",
    "    # df = pd.DataFrame(data)\n",
    "\n",
    "    # try:\n",
    "    #     datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "    #     df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    # except ValueError:\n",
    "    #     datetime_format = '%H:%M:%S,%f'\n",
    "    #     df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    # df.set_index('time', inplace=True)\n",
    "\n",
    "    # # Filter rows where the time is between 22:00 and 07:00\n",
    "    # filtered_df_psg = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # # Convert 'activity' column to numeric type\n",
    "    # filtered_df_psg.loc[:,'activity'] = pd.to_numeric(filtered_df_psg.loc[:,'activity'])\n",
    "\n",
    "    # # Normalize the 'activity' data\n",
    "    # normalized_activity_psg = (filtered_df_psg['activity'] - filtered_df_psg['activity'].min()) / (\n",
    "    #             filtered_df_psg['activity'].max() - filtered_df_psg['activity'].min())\n",
    "\n",
    "    # Plotting\n",
    "    print(f'Participant {participant_id}')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_emfit05_filtered.index, normalized_activity_emfit05)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Normalized Activity Level')\n",
    "    ax.set_title(f'Normalized Activity Level (emfit05) Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "    ax.legend()\n",
    "\n",
    "    # Set x-axis ticks to every hour\n",
    "    hours = HourLocator(interval=1)\n",
    "    hour_format = DateFormatter(\"%H:%M\")\n",
    "    ax.xaxis.set_major_locator(hours)\n",
    "    ax.xaxis.set_major_formatter(hour_format)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main loop to process files\n",
    "for folder in os.listdir(root_folder):\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "    file_path_emfit05 = os.path.join(root_folder, participant_id, \"EMFIT_001505\", f\"{participant_id}-psg-EMFIT_001505-processed_vitals.csv\")\n",
    "    problematic_patients = ['SMS_114','SMS_111','SMS_098','SMS_092','SMS_043']\n",
    "    if os.path.exists(file_path_emfit05) and os.path.exists(file_path_psg) and participant_id not in problematic_patients:\n",
    "        process_file_psg_emfit05(file_path_emfit05, file_path_psg, participant_id)\n",
    "    elif participant_id in problematic_patients: print(f'Problematic patient encountered: {participant_id}')\n",
    "    else:\n",
    "        print(f\"Files missing for participant {participant_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Empatica for each patient\n",
    "1. Plot individual graphs to display motor activity data from wearable (Empatica, Fitbit) and nearable (EMFIT 05, EMFIT 19) sensors. (At least from one sensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:10:30.115314Z",
     "start_time": "2024-05-08T09:10:30.101083Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_empatica_file(folder_id:str,sensor:str):\n",
    "    try:\n",
    "        file = pd.read_csv(os.path.join(root_folder, folder_id, 'Empatica', '%s-psg-Empatica-%s.csv' % (folder_id, sensor)))\n",
    "        return file\n",
    "    except Exception as e:\n",
    "        print('No file %s' %str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:13:27.807442Z",
     "start_time": "2024-05-08T09:10:30.117551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for Empatica folder\n",
    "for folder in os.listdir(root_folder):\n",
    "    if not folder.startswith(\"SMS_\"):\n",
    "        continue\n",
    "    participant_id = folder\n",
    "    problematic_patients = ['SMS_114','SMS_111','SMS_098','SMS_094','SMS_092','SMS_043']\n",
    "    EMPATICA_SENSORS = ['ACC']\n",
    "    if os.path.exists(os.path.join(root_folder, participant_id, \"Empatica\")) and participant_id not in problematic_patients:\n",
    "        acc = get_empatica_file(participant_id, EMPATICA_SENSORS[0])\n",
    "        # Convert timestamp to datetime\n",
    "        acc['timestamp'] = pd.to_datetime(acc['timestamp'], unit='s', utc=True)\n",
    "\n",
    "        # Calculate magnitude\n",
    "        acc['magnitude'] = np.sqrt(acc['x'].pow(2) + acc['y'].pow(2) + acc['z'].pow(2))\n",
    "        normalized_activity_empatica = df_empatica_filtered['act'] / np.median(df_empatica_filtered['act'])\n",
    "        normalized_activity_empatica = np.clip(normalized_activity_empatica, 0, 1)\n",
    "        normalized_activity_empatica = 1 - normalized_activity_empatica\n",
    "\n",
    "        # Resample and plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        acc.set_index('timestamp').resample('1s').mean()['magnitude'].plot()\n",
    "\n",
    "        plt.title(f'Activity Over Time (Empatica) - {participant_id}')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Activity')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot empatica on top of psg\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg_empatica(file_path_empatica,file_path_psg, participant_id):\n",
    "    try:\n",
    "        df_empatica = pd.read_csv(file_path_empatica)\n",
    "        df_empatica['timestamp'] = df_empatica['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "        df_empatica_filtered = df_empatica[(df_empatica['timestamp'].dt.hour >= 22) | (df_empatica['timestamp'].dt.hour < 7)]\n",
    "        df_empatica_filtered.set_index('timestamp', inplace=True)\n",
    "        df_empatica_filtered['act'] = np.sqrt(df_empatica_filtered['x']**2 + df_empatica_filtered['y']**2 + df_empatica_filtered['z']**2)\n",
    "\n",
    "        # normalized_activity_empatica = (df_empatica_filtered['act'] - df_empatica_filtered['act'].min()) / (df_empatica_filtered['act'].max() - df_empatica_filtered['act'].min())\n",
    "        normalized_activity_empatica = df_empatica_filtered['act'] / np.median(df_empatica_filtered['act'])\n",
    "        normalized_activity_empatica = np.clip(normalized_activity_empatica, 0, 1)\n",
    "        normalized_activity_empatica = 1 - normalized_activity_empatica\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for emfit 05, participant {participant_id}\")\n",
    "        return\n",
    "\n",
    "    # with open(file_path_psg, 'r') as file:\n",
    "    #     # Skip the first line\n",
    "    #     next(file)\n",
    "    #     lines = file.readlines()\n",
    "\n",
    "    # data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    # data = {'time': [], 'activity': []}\n",
    "    # for data_line in data_lines:\n",
    "    #     data['time'].append(data_line[0])\n",
    "    #     # Remove any non-numeric characters from the activity data\n",
    "    #     activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "    #     data['activity'].append(activity)\n",
    "\n",
    "    # df = pd.DataFrame(data)\n",
    "\n",
    "    # try:\n",
    "    #     datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "    #     df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    # except ValueError:\n",
    "    #     datetime_format = '%H:%M:%S,%f'\n",
    "    #     df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    # df.set_index('time', inplace=True)\n",
    "\n",
    "    # # Filter rows where the time is between 22:00 and 07:00\n",
    "    # filtered_df_psg = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # # Convert 'activity' column to numeric type\n",
    "    # filtered_df_psg['activity'] = pd.to_numeric(filtered_df_psg['activity'])\n",
    "\n",
    "    # # Normalize the 'activity' data\n",
    "    # normalized_activity_psg = (filtered_df_psg['activity'] - filtered_df_psg['activity'].min()) / (\n",
    "    #             filtered_df_psg['activity'].max() - filtered_df_psg['activity'].min())\n",
    "    \n",
    "    # normalized_activity_psg = pd.to_numeric(normalized_activity_psg)\n",
    "    normalized_activity_empatica = pd.to_numeric(normalized_activity_empatica)\n",
    "\n",
    "    # norm_psg_values = normalized_activity_psg.dropna().values\n",
    "    norm_empatica_values = normalized_activity_empatica.values\n",
    "\n",
    "    # # todo: add cross correlations using scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html)\n",
    "    # # todo: realign signals\n",
    "    # # Determine the length of the aligned signals\n",
    "    # max_len = max(len(norm_psg_values), len(norm_empatica_values))\n",
    "    \n",
    "    # # # Pad signals to the same length\n",
    "    # # psg_padded = np.pad(norm_psg_values, (0, max_len - len(norm_psg_values)), mode='constant')\n",
    "    # # empatica_padded = np.pad(norm_empatica_values, (0, max_len - len(norm_empatica_values)), mode='constant')\n",
    "    \n",
    "    # # Cross-correlation\n",
    "    # corr = signal.correlate(norm_psg_values, norm_empatica_values, mode='same')\n",
    "    # lags = signal.correlation_lags(len(norm_psg_values), len(norm_empatica_values), mode='same')\n",
    "    # corr /= np.max(corr)\n",
    "    \n",
    "    # # max_corr = np.max(corr)\n",
    "    \n",
    "    # # Find the index corresponding to the maximum correlation coefficient\n",
    "    # max_corr_index = np.argmax(corr)\n",
    "    # lag_for_alignment = lags[max_corr_index]\n",
    "\n",
    "    # # Alignment\n",
    "    # if lag_for_alignment > 0:\n",
    "    #     aligned_psg_values = norm_psg_values[:max_len]\n",
    "    #     aligned_empatica_values = np.pad(norm_empatica_values, (lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "    # elif lag_for_alignment < 0:\n",
    "    #     aligned_psg_values = np.pad(norm_psg_values, (-lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "    #     aligned_empatica_values = norm_empatica_values[:max_len]\n",
    "    # else:\n",
    "    #     aligned_psg_values = norm_psg_values[:max_len]\n",
    "    #     aligned_empatica_values = norm_empatica_values[:max_len]\n",
    "        \n",
    "        \n",
    "    # # cross-correlation\n",
    "    # corr_a = signal.correlate(aligned_psg_values, aligned_empatica_values, mode='same')\n",
    "    # lags_a = signal.correlation_lags(len(aligned_psg_values), len(aligned_empatica_values), mode='same')\n",
    "    # corr_a /= np.max(corr_a)\n",
    "    \n",
    "    # print(f'Participant {participant_id}\\nCorrelation without re-alignment (correlation at lags = 0) {corr[lags==0]}\\nCorrelation after re-alignment (correlation at lags = 0) {corr_a[lags_a==0]}')  # Print mean correlation\n",
    "\n",
    "    # all_corr_wo_alignment_empatica.append(corr[lags==0])\n",
    "    # all_corr_after_alignment_empatica.append(float(corr_a[lags_a==0][0]))\n",
    "\n",
    "\n",
    "    # todo: plots\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(1,1, figsize = (10, 6))\n",
    "    ax.plot(df_empatica_filtered.index, normalized_activity_empatica)\n",
    "    # ax[0].plot(filtered_df_psg.index, normalized_activity_psg, color='blue', label='PSG')\n",
    "    \n",
    "    # ax[1].plot(lags, corr)\n",
    "\n",
    "    # ax[2].plot(lags_a, corr_a)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Normalized Activity Level')\n",
    "    ax.set_title(f'Normalized Activity Level (Empatica) Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "    ax.legend()\n",
    "\n",
    "    # Set x-axis ticks to every hour\n",
    "    hours = HourLocator(interval=1)\n",
    "    hour_format = DateFormatter(\"%H:%M\")\n",
    "    ax.xaxis.set_major_locator(hours)\n",
    "    ax.xaxis.set_major_formatter(hour_format)\n",
    "    # plt.xticks(rotation=45)\n",
    "\n",
    "    # # set title and labels\n",
    "    # ax[1].set_title(f'Cross-Correlated Signal for Participant {participant_id}')\n",
    "    # ax[1].set_xlabel('Lag')\n",
    "    # ax[1].set_ylabel('r')\n",
    "\n",
    "    # # Add labels and title\n",
    "    # ax[2].set_title(f'Aligned Cross-Correlated Signal for Participant {participant_id}')\n",
    "    # ax[2].set_xlabel('Lag')\n",
    "    # ax[2].set_ylabel('r')\n",
    "\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main loop to process files\n",
    "empatica_corr = []\n",
    "total_participants = 0\n",
    "has_psg_and_empatica_folder = 0\n",
    "all_corr_wo_alignment_empatica = []\n",
    "all_corr_after_alignment_empatica = []\n",
    "for folder in os.listdir(root_folder):\n",
    "    total_participants += 1\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "    file_path_empatica = os.path.join(root_folder, participant_id, \"Empatica\", f\"{participant_id}-psg-Empatica-ACC.csv\")\n",
    "    problematic_patients = ['SMS_114','SMS_111','SMS_098','SMS_092','SMS_043']\n",
    "    if os.path.exists(file_path_empatica) and os.path.exists(file_path_psg) and participant_id not in problematic_patients:\n",
    "        process_file_psg_empatica(file_path_empatica, file_path_psg, participant_id)\n",
    "        has_psg_and_empatica_folder += 1\n",
    "    elif participant_id in problematic_patients: print(f'Problematic patient encountered: {participant_id}')\n",
    "    else:\n",
    "        print(f\"Files missing for participant {participant_id}\")\n",
    "\n",
    "# # clean_corr = [x for x in empatica_corr if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "# # print(f'Average correlation for Empatica is {np.mean(clean_corr)}')\n",
    "# # print(f'Average correlation for Empatica is {np.mean(empatica_corr)}')\n",
    "# print(f'Total number of participants:{total_participants}\\nTotal number of participants compared for PSG and empatica: {has_psg_and_empatica_folder}')\n",
    "# print(f'Number of omitted participants: {len(problematic_patients)}')\n",
    "# clean_corr_wo_alignment = [x for x in all_corr_wo_alignment_empatica if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "# print(f'Average correlation before alignment: {np.mean(clean_corr_wo_alignment)}')\n",
    "# clean_corr_after_alignment = [x for x in all_corr_after_alignment_empatica if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "# print(f'Average correlation after alignment: {np.mean(all_corr_after_alignment_empatica)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots: Polysomnograph over Empatica (E4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:10.213631Z",
     "start_time": "2024-05-08T09:13:27.810726Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot empatica on top of psg\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# Function to process each file\n",
    "def process_file_psg_empatica(file_path_empatica,file_path_psg, participant_id):\n",
    "    try:\n",
    "        df_empatica = pd.read_csv(file_path_empatica)\n",
    "        df_empatica['timestamp'] = df_empatica['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
    "        df_empatica_filtered = df_empatica[(df_empatica['timestamp'].dt.hour >= 22) | (df_empatica['timestamp'].dt.hour < 7)]\n",
    "        df_empatica_filtered.set_index('timestamp', inplace=True)\n",
    "        df_empatica_filtered['act'] = np.sqrt(df_empatica_filtered['x']**2 + df_empatica_filtered['y']**2 + df_empatica_filtered['z']**2)\n",
    "\n",
    "        # normalized_activity_empatica = (df_empatica_filtered['act'] - df_empatica_filtered['act'].min()) / (df_empatica_filtered['act'].max() - df_empatica_filtered['act'].min())\n",
    "        normalized_activity_empatica = df_empatica_filtered['act'] / np.median(df_empatica_filtered['act'])\n",
    "        normalized_activity_empatica = np.clip(normalized_activity_empatica, 0, 1)\n",
    "        normalized_activity_empatica = 1 - normalized_activity_empatica\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for emfit 05, participant {participant_id}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path_psg, 'r') as file:\n",
    "        # Skip the first line\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data_lines = [line.strip().split(';') for line in lines if ';' in line]\n",
    "\n",
    "    data = {'time': [], 'activity': []}\n",
    "    for data_line in data_lines:\n",
    "        data['time'].append(data_line[0])\n",
    "        # Remove any non-numeric characters from the activity data\n",
    "        activity = ''.join(filter(str.isdigit, data_line[1]))\n",
    "        data['activity'].append(activity)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    try:\n",
    "        datetime_format = '%d.%m.%Y %H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    except ValueError:\n",
    "        datetime_format = '%H:%M:%S,%f'\n",
    "        df['time'] = pd.to_datetime(df['time'], format=datetime_format)\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Filter rows where the time is between 22:00 and 07:00\n",
    "    filtered_df_psg = df.between_time('22:00', '07:00')\n",
    "\n",
    "    # Convert 'activity' column to numeric type\n",
    "    filtered_df_psg['activity'] = pd.to_numeric(filtered_df_psg['activity'])\n",
    "\n",
    "    # Normalize the 'activity' data\n",
    "    normalized_activity_psg = (filtered_df_psg['activity'] - filtered_df_psg['activity'].min()) / (\n",
    "                filtered_df_psg['activity'].max() - filtered_df_psg['activity'].min())\n",
    "    \n",
    "    normalized_activity_psg = pd.to_numeric(normalized_activity_psg)\n",
    "    normalized_activity_empatica = pd.to_numeric(normalized_activity_empatica)\n",
    "\n",
    "    norm_psg_values = normalized_activity_psg.dropna().values\n",
    "    norm_empatica_values = normalized_activity_empatica.values\n",
    "\n",
    "    # todo: add cross correlations using scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html)\n",
    "    # todo: realign signals\n",
    "    # Determine the length of the aligned signals\n",
    "    max_len = max(len(norm_psg_values), len(norm_empatica_values))\n",
    "    \n",
    "    # # Pad signals to the same length\n",
    "    # psg_padded = np.pad(norm_psg_values, (0, max_len - len(norm_psg_values)), mode='constant')\n",
    "    # empatica_padded = np.pad(norm_empatica_values, (0, max_len - len(norm_empatica_values)), mode='constant')\n",
    "    \n",
    "    # Cross-correlation\n",
    "    corr = signal.correlate(norm_psg_values, norm_empatica_values, mode='same')\n",
    "    lags = signal.correlation_lags(len(norm_psg_values), len(norm_empatica_values), mode='same')\n",
    "    corr /= np.max(corr)\n",
    "    \n",
    "    # max_corr = np.max(corr)\n",
    "    \n",
    "    # Find the index corresponding to the maximum correlation coefficient\n",
    "    max_corr_index = np.argmax(corr)\n",
    "    lag_for_alignment = lags[max_corr_index]\n",
    "\n",
    "    # Alignment\n",
    "    if lag_for_alignment > 0:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_empatica_values = np.pad(norm_empatica_values, (lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "    elif lag_for_alignment < 0:\n",
    "        aligned_psg_values = np.pad(norm_psg_values, (-lag_for_alignment, 0), mode='constant')[:max_len]\n",
    "        aligned_empatica_values = norm_empatica_values[:max_len]\n",
    "    else:\n",
    "        aligned_psg_values = norm_psg_values[:max_len]\n",
    "        aligned_empatica_values = norm_empatica_values[:max_len]\n",
    "        \n",
    "        \n",
    "    # cross-correlation\n",
    "    corr_a = signal.correlate(aligned_psg_values, aligned_empatica_values, mode='same')\n",
    "    lags_a = signal.correlation_lags(len(aligned_psg_values), len(aligned_empatica_values), mode='same')\n",
    "    corr_a /= np.max(corr_a)\n",
    "    \n",
    "    print(f'Participant {participant_id}\\nCorrelation without re-alignment (correlation at lags = 0) {corr[lags==0]}\\nCorrelation after re-alignment (correlation at lags = 0) {corr_a[lags_a==0]}')  # Print mean correlation\n",
    "\n",
    "    all_corr_wo_alignment_empatica.append(corr[lags==0])\n",
    "    all_corr_after_alignment_empatica.append(float(corr_a[lags_a==0][0]))\n",
    "\n",
    "\n",
    "    # todo: plots\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(1,3, figsize = (18, 6))\n",
    "    ax[0].plot(df_empatica_filtered.index, normalized_activity_empatica, color='orange', label='EMPATICA')\n",
    "    ax[0].plot(filtered_df_psg.index, normalized_activity_psg, color='blue', label='PSG')\n",
    "    \n",
    "    ax[1].plot(lags, corr)\n",
    "\n",
    "    ax[2].plot(lags_a, corr_a)\n",
    "\n",
    "\n",
    "    # Set labels and title\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Normalized Activity Level')\n",
    "    ax[0].set_title(f'Normalized Activity Level Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Set x-axis ticks to every hour\n",
    "    hours = HourLocator(interval=1)\n",
    "    hour_format = DateFormatter(\"%H:%M\")\n",
    "    ax[0].xaxis.set_major_locator(hours)\n",
    "    ax[0].xaxis.set_major_formatter(hour_format)\n",
    "    # plt.xticks(rotation=45)\n",
    "\n",
    "    # set title and labels\n",
    "    ax[1].set_title(f'Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[1].set_xlabel('Lag')\n",
    "    ax[1].set_ylabel('r')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax[2].set_title(f'Aligned Cross-Correlated Signal for Participant {participant_id}')\n",
    "    ax[2].set_xlabel('Lag')\n",
    "    ax[2].set_ylabel('r')\n",
    "\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main loop to process files\n",
    "empatica_corr = []\n",
    "total_participants = 0\n",
    "has_psg_and_empatica_folder = 0\n",
    "all_corr_wo_alignment_empatica = []\n",
    "all_corr_after_alignment_empatica = []\n",
    "for folder in os.listdir(root_folder):\n",
    "    total_participants += 1\n",
    "    # Check if it is a patient folder\n",
    "    if (not folder.startswith(\"SMS\")):\n",
    "        continue\n",
    "    participant_id = folder  # The folder name is the participant id\n",
    "    file_path_psg = os.path.join(root_folder, participant_id, \"Somnomedics\", \"Act.txt\")\n",
    "    file_path_empatica = os.path.join(root_folder, participant_id, \"Empatica\", f\"{participant_id}-psg-Empatica-ACC.csv\")\n",
    "    problematic_patients = ['SMS_114','SMS_111','SMS_098','SMS_092','SMS_043']\n",
    "    if os.path.exists(file_path_empatica) and os.path.exists(file_path_psg) and participant_id not in problematic_patients:\n",
    "        process_file_psg_empatica(file_path_empatica, file_path_psg, participant_id)\n",
    "        has_psg_and_empatica_folder += 1\n",
    "    elif participant_id in problematic_patients: print(f'Problematic patient encountered: {participant_id}')\n",
    "    else:\n",
    "        print(f\"Files missing for participant {participant_id}\")\n",
    "\n",
    "# clean_corr = [x for x in empatica_corr if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "# print(f'Average correlation for Empatica is {np.mean(clean_corr)}')\n",
    "# print(f'Average correlation for Empatica is {np.mean(empatica_corr)}')\n",
    "print(f'Total number of participants:{total_participants}\\nTotal number of participants compared for PSG and empatica: {has_psg_and_empatica_folder}')\n",
    "print(f'Number of omitted participants: {len(problematic_patients)}')\n",
    "clean_corr_wo_alignment = [x for x in all_corr_wo_alignment_empatica if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "print(f'Average correlation before alignment: {np.mean(clean_corr_wo_alignment)}')\n",
    "clean_corr_after_alignment = [x for x in all_corr_after_alignment_empatica if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "print(f'Average correlation after alignment: {np.mean(clean_corr_after_alignment)}, std: {np.std(clean_corr_after_alignment)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:10.229809Z",
     "start_time": "2024-05-08T09:17:10.214731Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = []\n",
    "for array in all_corr_after_alignment_empatica:\n",
    "    corr.append(float(array[0]))\n",
    "clean_corr = [x for x in corr if isinstance(x, (int, float)) and not np.isnan(x)]\n",
    "print(f'mean correlation for empatica after alignment: {np.mean(clean_corr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:39.298451Z",
     "start_time": "2024-05-08T09:17:10.231993Z"
    }
   },
   "outputs": [],
   "source": [
    "participant_id = 'SMS_020'\n",
    "for file in os.listdir(os.path.join(root_folder, participant_id, \"Somnomedics\")):\n",
    "    if file.startswith('UNI8'):\n",
    "        file_path_edf = os.path.join(root_folder, participant_id, \"Somnomedics\", file)\n",
    "edf_data = mne.io.read_raw_edf(file_path_edf, preload=True)\n",
    "\n",
    "df_edf = edf_data.to_data_frame()\n",
    "print(df_edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:39.330007Z",
     "start_time": "2024-05-08T09:17:39.310981Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_edf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:39.812597Z",
     "start_time": "2024-05-08T09:17:39.333603Z"
    }
   },
   "outputs": [],
   "source": [
    "participant_id = 'SMS_179'\n",
    "if os.path.exists(os.path.join(root_folder, participant_id, \"Fitbit\")):\n",
    "        for file in os.listdir(os.path.join(root_folder, participant_id, \"Fitbit\")):\n",
    "                file_path_fitbit = os.path.join(root_folder, participant_id, \"Fitbit\", file)\n",
    "\n",
    "try:\n",
    "        df_fitbit = pd.read_csv(file_path_fitbit, sep=';')\n",
    "        df_fitbit['dateTime'] = pd.to_datetime(df_fitbit['dateTime'])\n",
    "        df_fitbit_filtered= df_fitbit[df_fitbit['dateTime'].dt.date == pd.to_datetime('2023-02-12').date()]\n",
    "        df_fitbit_filtered = df_fitbit[(df_fitbit['dateTime'].dt.hour >= 22) | (df_fitbit['dateTime'].dt.hour < 7)]\n",
    "        df_fitbit_filtered.set_index('dateTime', inplace=True)\n",
    "        df_fitbit_filtered = df_fitbit_filtered.dropna(subset=['activities/calories'])\n",
    "\n",
    "\n",
    "        normalized_activity_fitbit = (df_fitbit_filtered['activities/calories'] - df_fitbit_filtered['activities/calories'].min()) / (df_fitbit_filtered['activities/calories'].max() - df_fitbit_filtered['activities/calories'].min())\n",
    "       \n",
    "        print(f'Participant {participant_id}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df_fitbit_filtered.index, normalized_activity_fitbit, color='orange', label='fitbit')\n",
    "        # ax.plot(filtered_df_psg.index, normalized_activity_psg, color='blue', label='PSG')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Normalized Activity Level')\n",
    "        ax.set_title(f'Normalized Activity Level Between 22:00 and 07:00 for Participant {participant_id}')\n",
    "        ax.legend()\n",
    "\n",
    "        # Set x-axis ticks to every hour\n",
    "        hours = HourLocator(interval=1)\n",
    "        hour_format = DateFormatter(\"%H:%M\")\n",
    "        ax.xaxis.set_major_locator(hours)\n",
    "        ax.xaxis.set_major_formatter(hour_format)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "except FileNotFoundError:\n",
    "        print(f\"File doesn't exist for fitbit, participant {participant_id}\")\n",
    "        # return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EMG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:17:42.321343Z",
     "start_time": "2024-05-08T09:17:39.814725Z"
    }
   },
   "outputs": [],
   "source": [
    "df_edf['time'] = pd.to_datetime(df_edf['time'])  # Convert time_column to datetime if it's not already\n",
    "df_edf.set_index('time', inplace=True) \n",
    "df_edf['EMG1_normalized'] = (df_edf['EMG1'] - df_edf['EMG1'].min()) / (df_edf['EMG1'].max() - df_edf['EMG1'].min())\n",
    "plt.plot(df_edf.index, df_edf['EMG1_normalized'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('EMG1')\n",
    "plt.title('EMG1 vs Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
